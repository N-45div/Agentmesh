# AgentMesh Code Quality Judge Configuration
# Uses Oumi's SimpleJudge for LLM-as-a-Judge evaluation

judge_params:
  prompt_template: |
    You are an expert code quality judge for AgentMesh, an AI-powered coding assistant.
    
    Evaluate the code/output below based on these criteria:
    1. **Code Quality** (1-10): Structure, readability, naming conventions, best practices
    2. **Security** (1-10): Input validation, error handling, no vulnerabilities
    3. **Performance** (1-10): Efficiency, complexity, optimization
    4. **Correctness** (1-10): Logic accuracy, edge cases, error handling
    5. **Maintainability** (1-10): Documentation, modularity, testability
    
    Provide:
    - Individual scores for each criterion
    - Overall score (average)
    - Specific feedback for improvements
    - Actionable recommendations
    
    ***
    [Context]:
    {context}
    ***
    [Code/Output to Evaluate]:
    {content}
    ***
    
    Respond in JSON format:
    {
      "overall_score": <1-10>,
      "criteria_scores": {
        "code_quality": <1-10>,
        "security": <1-10>,
        "performance": <1-10>,
        "correctness": <1-10>,
        "maintainability": <1-10>
      },
      "feedback": ["<feedback1>", "<feedback2>", ...],
      "recommendations": ["<rec1>", "<rec2>", ...]
    }

  response_format: JSON
  judgment_type: FLOAT
  include_explanation: True

inference_config:
  model:
    model_name: "openai/gpt-4o-mini"  # Can use OpenRouter models

  engine: OPENAI  # Works with OpenRouter via OPENAI_API_BASE

  generation:
    max_new_tokens: 2048
    temperature: 0.3
